{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Cleaning Data\n",
    "\n",
    "This chapter, you will understand how to prepare your data for analysis and diagnose data for problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Overview Methods\n",
    "\n",
    "`df.head()`, `df.tail()`\n",
    "\n",
    "Use it to have quick look on data.\n",
    "\n",
    "`df.shape`\n",
    "\n",
    "You can check many columns and rows in your dasta.\n",
    "\n",
    "`df.columns`\n",
    "\n",
    "You can see what's the column name.\n",
    "\n",
    "`df.info()`\n",
    "\n",
    "You can check column types and non-null values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pm_2_5_file_path = './res/observation_pm_2_5.csv'\n",
    "\n",
    "# Some rows has value 'NR', we treat it as null\n",
    "df = pd.read_csv(pm_2_5_file_path, na_values='NR')\n",
    "\n",
    "print(df.head(2))\n",
    "\n",
    "print(df.tail(2))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Exploratory Methods\n",
    "\n",
    "`df.value_counts()`\n",
    "\n",
    "Frequency counts for categorical data.\n",
    "\n",
    "`df.describe()`\n",
    "\n",
    "Have a quick understanding of your column values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Date'].value_counts().head())\n",
    "\n",
    "print(df['Observation Item'].value_counts().head())\n",
    "\n",
    "print(df['0'].describe())\n",
    "\n",
    "pm_2_5_df = df[df.iloc[:, 1] == 'PM2.5']\n",
    "\n",
    "print(pm_2_5_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: The world countreis life expectancy number is in path `./res/g1800.csv`, please read it to DataFrame `g1800_df` with Pandas and have a look on data.\n",
    "\n",
    "Hint: When you call method `info()`, please only select first 100 columns, otherwise you will not get non-null count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Explore Data\n",
    "\n",
    "##### Histogram\n",
    "\n",
    "Use it to look at frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pm_2_5_df['0'].plot(kind='hist', bins=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Box Plots\n",
    "\n",
    "Visualize basic summary statistics.\n",
    "\n",
    "* Outliers\n",
    "* Min/Max\n",
    "* 25th,50th,75th percentiles\n",
    "\n",
    "After you find outliers values, you can remove those rows(call `df.drop(indice_name_list)`) if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.boxplot(column=['0'], by='Observation Item', figsize=(18, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Plots\n",
    "\n",
    "Use it to see correlation between two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "o3_df = df[df.iloc[:, 1] == 'O3']\n",
    "pm10_df = df[df.iloc[:, 1] == 'PM10']\n",
    "\n",
    "print(pm_2_5_df.head())\n",
    "print(o3_df.head())\n",
    "print(pm10_df.head())\n",
    "\n",
    "plt.scatter(pm_2_5_df['0'], o3_df['0'])\n",
    "plt.xlabel('pm 2.5')\n",
    "plt.ylabel('O3')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(pm_2_5_df['0'], pm10_df['0'])\n",
    "plt.xlabel('pm 2.5')\n",
    "plt.ylabel('PM 10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Please visualize DataFrame `g1800_df` and see what's your finding.\n",
    "\n",
    "* See histgram chart using data in year 1800.\n",
    "* See scatter chart using data in year 1800 and in year 1899, if they have correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data\n",
    "\n",
    "Tidy data makes it better for analysis and easier to fix column data problems.\n",
    "\n",
    "##### Priciples of Tidy Data\n",
    "\n",
    "* Columns represents separate variables\n",
    "* Row represents individual observations\n",
    "* Observational units forms tabls\n",
    "\n",
    "Use `pd.melt()` to reshape DataFrame. Put column names which to be stayed in columns into argument `id_vars`, others will be melted into columns `variable`, `value`. You can rename these two column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "melt_df = pd.melt(frame=df, id_vars=['Date', 'Observation Item'], var_name='hour', value_name='value')\n",
    "\n",
    "print(melt_df.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: The Format of DataFrame `g1800_df` is not good to analyze, please convert into tidy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Data\n",
    "\n",
    "It is the opposite of `melting`. In melting, columns are turned into rows. In Pivoting, rows are turned into columns.\n",
    "\n",
    "It could convert analysis friendly shape to report friendly shape.\n",
    "\n",
    "Use `pd.pivot_table()` to reshape DataFrame. Put variables which to be kept in columns in `index`, fill `columns` with other variables, and fill `values` with values\n",
    "\n",
    "`You will not able to pivot data if you have duplicated entries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Column order of pivot table will be sorted by column, convert type to number to have expected order\n",
    "melt_df['hour'] = melt_df['hour'].apply(pd.to_numeric)\n",
    "\n",
    "print(melt_df.head())\n",
    "\n",
    "pivot_df = melt_df.pivot_table(index=['Date', 'Observation Item'], columns='hour', values='value')\n",
    "\n",
    "print(pivot_df.head())\n",
    "\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "print(pivot_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use argument `aggfunc` to remove duplicates or calculate aggregate result, available functions are `np.sum`, `np.mean`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(melt_df.head())\n",
    "\n",
    "pivot_agg_df = melt_df.pivot_table(index=['Date', 'Observation Item'], values='value', aggfunc=np.mean)\n",
    "\n",
    "print(pivot_agg_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenation\n",
    "\n",
    "`pd.concat` is used for concatenate multiple dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "au_2018_df = pd.DataFrame(np.array([['2018-01-01', '13000'], ['2018-01-02', '13645']]))\n",
    "au_2018_df.columns = ['year', 'active_users']\n",
    "print(au_2018_df)\n",
    "\n",
    "au_2017_df = pd.DataFrame(np.array([['2017-01-01', '10329'], ['2017-01-02', '10986']]))\n",
    "au_2017_df.columns = ['year', 'active_users']\n",
    "print(au_2017_df)\n",
    "\n",
    "au_concat_df = pd.concat([au_2018_df, au_2017_df])\n",
    "\n",
    "print(au_concat_df.shape)\n",
    "print(au_concat_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate data for different axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(au_2018_df)\n",
    "\n",
    "ru_2018_df = pd.DataFrame(np.array([['2018-01-01', '256'], ['2018-01-02', '283']]))\n",
    "ru_2018_df.columns = ['year', 'register_users']\n",
    "print(ru_2018_df)\n",
    "\n",
    "au_ru_concat_df = pd.concat([au_2018_df, ru_2018_df], axis=1)\n",
    "print(au_ru_concat_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Globbing\n",
    "\n",
    "you can use `pd.blob()` if you have many files need to be concatenated.\n",
    "\n",
    "Use Pattern match for file names.\n",
    "\n",
    "Wildcards:\n",
    "\n",
    "* `*`: match any csv files: *.csv \n",
    "* `?`: match any single characters(A-z, 0-9): file_?.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "csv_files = glob.glob('./res/*.csv')\n",
    "print(csv_files)\n",
    "\n",
    "yearly_registers_files = glob.glob('./res/yearly_registers*')\n",
    "print(yearly_registers_files)\n",
    "\n",
    "yearly_registers_files = glob.glob('./res/yearly_registers.???')\n",
    "print(yearly_registers_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `glob`, you can concatenate dataframes by following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "registers_files = glob.glob('./res/monthly_registers_*.csv')\n",
    "\n",
    "\n",
    "register_dfs = []\n",
    "\n",
    "for file in registers_files:\n",
    "    print(file)\n",
    "    register_df = pd.read_csv(file)\n",
    "    register_dfs.append(register_df)\n",
    "\n",
    "register_concat_df = pd.concat(register_dfs)\n",
    "print(register_concat_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: We have year 1800, 1900, 2000 life expectancy number in different files, `./res/g1800.csv`,  `./res/g1900.csv`,  `./res/g2000.csv`, please tidy those data and concatenate it into DataFrame `life_expectancy_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging Data\n",
    "\n",
    "Similar to `join` operation in SQL.\n",
    "\n",
    "Use `pd.merge()` to merge DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "countries_registers_df = pd.read_csv('./res/country_registers_2017.csv')\n",
    "countries_df = pd.read_csv('./res/countries.csv')\n",
    "\n",
    "print(countries_registers_df.head())\n",
    "print(countries_df.head())\n",
    "\n",
    "merged_df = pd.merge(left=countries_registers_df, right=countries_df, left_on='countryid', right_on='countryid')\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting Data Type\n",
    "\n",
    "You can convert data type to your expected one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "registers_2017_df = pd.read_csv('./res/monthly_registers_2017.csv')\n",
    "print(registers_2017_df.head())\n",
    "print(registers_2017_df.dtypes)\n",
    "\n",
    "registers_2017_df['amount'] = registers_2017_df['amount'].astype(float)\n",
    "print(registers_2017_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a column is a category field, you can use `category` type to save memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pm_2_5_file_path = './res/observation_pm_2_5.csv'\n",
    "\n",
    "df = pd.read_csv(pm_2_5_file_path)\n",
    "\n",
    "print(df.dtypes.head(5))\n",
    "print(df.info())\n",
    "\n",
    "df['Observation Item'] = df['Observation Item'].astype('category')\n",
    "\n",
    "print(df.dtypes.head(5))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you column contains other type instead of numeric, it will show error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['0'].head(18))\n",
    "\n",
    "print(df['0'].value_counts().head())\n",
    "\n",
    "df['0'] = pd.to_numeric(df['0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add argument `errors='coerce'` can solve this problem.\n",
    "\n",
    "The value cannot be converted into number will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['0'] = pd.to_numeric(df['0'], errors='coerce')\n",
    "\n",
    "print(df['0'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parsing string with Regular Expression\n",
    "\n",
    "You can search online for Regular Expression Syntax, it will be not covered here.\n",
    "\n",
    "Use `.str.match()` to check if field matchs a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Observation Item'].head(18))\n",
    "\n",
    "is_match_pm = df['Observation Item'].str.match('PM.+')\n",
    "\n",
    "print(df[is_match_pm].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.str.extract()` to extract match string, and use `.str.extractall` to extract match string groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Observation Item'].head(18))\n",
    "\n",
    "print(df['Observation Item'].str.extract('PM([\\d.]+)', expand=True).head(18))\n",
    "print(df['Observation Item'].str.extractall('(.)').head(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using function to clean data\n",
    "\n",
    "\n",
    "You can define a function with following syntax\n",
    "\n",
    "```\n",
    "def function_name():\n",
    "    funciton_expression\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello_world(name):\n",
    "    return \"Hello, \" + name\n",
    "\n",
    "print(hello_world('Will'))\n",
    "print(hello_world('Taiwan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write custom function and use `df.apply()` to make it work in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "item_price_df = pd.DataFrame(np.array([['Hot Dog', 30], ['bread', 45]]))\n",
    "item_price_df.columns = ['item', 'price']\n",
    "\n",
    "print(item_price_df)\n",
    "\n",
    "def add_currency(price, currency=None):\n",
    "    return currency + str(price)\n",
    "\n",
    "item_price_df['price_with_currency'] = item_price_df.price.apply(add_currency, currency='GBP')\n",
    "\n",
    "print(item_price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Lambda function to clean data\n",
    "\n",
    "`Lambda function` is a Python powerful feature which could help you to iterate over list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(item_price_df)\n",
    "\n",
    "currency = 'NTD'\n",
    "item_price_df['price_with_currency_2'] = item_price_df.price.apply(lambda x: currency + str(x))\n",
    "\n",
    "print(item_price_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Duplicate and Missing Value\n",
    "\n",
    "You can call `df.drop_duplicates()` to drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dup_df = pd.DataFrame(np.array([['dog'], ['dog'], ['cat']]))\n",
    "dup_df.columns = ['animal']\n",
    "\n",
    "print(dup_df)\n",
    "\n",
    "dup_df = dup_df.drop_duplicates()\n",
    "\n",
    "print(dup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call `df.dropna()` to remove rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pm_2_5_file_path = './res/observation_pm_2_5.csv'\n",
    "\n",
    "# Some rows has value 'NR', we treat it as null\n",
    "df = pd.read_csv(pm_2_5_file_path, na_values='NR')\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "drop_na_df = df.dropna()\n",
    "\n",
    "print(drop_na_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also fill missing value with other value using `df.fillna(value)`, such as average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.mean()\n",
    "\n",
    "fill_na_df = df.copy()\n",
    "print(fill_na_df.info())\n",
    "\n",
    "for i in range(0, 24):\n",
    "    print('mean of hour ' + str(i) + ' is ' + str(means[i]))\n",
    "    # i+2 because column has Date and Observation Item columns\n",
    "    fill_na_df[str(i)] = fill_na_df[str(i)].fillna(means[i])\n",
    "    \n",
    "print(fill_na_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assert data\n",
    "\n",
    "You can use `assert` syntax to make sure your data is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1 == 1\n",
    "\n",
    "assert 1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pd.notnull(df)` to check if it contains null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "assert pd.notnull(fill_na_df).all().all()\n",
    "\n",
    "assert pd.notnull(df).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Now, you can play DataFrame `life_expectancy_df` whatever you like. such as show line chart of life expectancy in specific country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
